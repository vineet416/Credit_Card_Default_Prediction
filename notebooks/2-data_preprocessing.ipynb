{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06f28ed",
   "metadata": {},
   "source": [
    "- Based on data exploration, now we will perform some data cleaning and preprocessing steps to prepare the dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c20a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the path to the datasets folder\n",
    "dataset_path = os.path.join(data_dir, \"UCI_Credit_Card.csv\")\n",
    "\n",
    "# Loading the CSV file into a DataFrame\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Displaying the first few rows to confirm loading\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c80da",
   "metadata": {},
   "source": [
    "# **1. Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f564847",
   "metadata": {},
   "source": [
    "## **1.1 Handling Missing Values**\n",
    "- We will handle the following data cleaning tasks based on the observations and assumptions from the data exploration phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling missing values in Education_Level and Marital_Status\n",
    "print(f\"Count of 0 values in Education_Level: {len(df[df['Education_Level'] == 0])}\")\n",
    "print(f\"Count of 0 values in Marital_Status: {len(df[df['Marital_Status'] == 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc1c0b3",
   "metadata": {},
   "source": [
    "- Missing values are very less so we will handle them by replacing them with appropriate values:\n",
    "    - **Education_Level**: Replace 0 with 5 (unknown).\n",
    "    - **Marital_Status**: Replace 0 with the mode of Marital_Status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mode for Marital_Status\n",
    "marital_mode = df['Marital_Status'].mode()[0]\n",
    "print(f\"\\nMode of Marital_Status: {marital_mode}\")\n",
    "\n",
    "# Replacing 0s in Marital_Status with mode\n",
    "df['Marital_Status'] = df['Marital_Status'].replace(0, marital_mode)\n",
    "\n",
    "# Replacing 0s in Education_Level with 5 (unknown)\n",
    "df['Education_Level'] = df['Education_Level'].replace(0, 5)\n",
    "\n",
    "# Verifying correction\n",
    "print(\"\\nCount of 0 values after handling:\")\n",
    "print(f\"Education_Level: {len(df[df['Education_Level'] == 0])}\")\n",
    "print(f\"Marital_Status: {len(df[df['Marital_Status'] == 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596a188",
   "metadata": {},
   "source": [
    "- We replaced 0s in Education_Level with 5(unknown).\n",
    "- We replaced 0s in Marital_Status with the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100ea94",
   "metadata": {},
   "source": [
    "## **1.2 Handling Inconsistencies in Categorical Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d021a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display counts (5,6) unknown in Education_Level\n",
    "print(f\"Education_Level 5: {len(df[df['Education_Level'] == 5])}\")\n",
    "print(f\"Education_Level 6: {len(df[df['Education_Level'] == 6])}\")\n",
    "\n",
    "# Replacing 6 in Education_Level with 5 (unknown)\n",
    "df['Education_Level'] = df['Education_Level'].replace(6, 5)\n",
    "\n",
    "# Display the updated counts\n",
    "print(f\"Updated Education_Level 5: {len(df[df['Education_Level'] == 5])}\")\n",
    "print(f\"Updated Education_Level 6: {len(df[df['Education_Level'] == 6])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c1b5d4",
   "metadata": {},
   "source": [
    "- We replaced 6(unknown) in Education_Level with 5(unknown) as both are (unknown) according to the documentation and as per our assumption to maintain consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8cd898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected value ranges based on documentation and assumptions\n",
    "expected_ranges = {\n",
    "    'Gender': [1, 2],\n",
    "    'Education_Level': [1, 2, 3, 4, 5],\n",
    "    'Marital_Status': [1, 2, 3],\n",
    "    'Sept_Pay': list(range(-2, 10)),\n",
    "    'Aug_Pay': list(range(-2, 10)),\n",
    "    'July_Pay': list(range(-2, 10)),\n",
    "    'June_Pay': list(range(-2, 10)),\n",
    "    'May_Pay': list(range(-2, 10)),\n",
    "    'Apr_Pay': list(range(-2, 10)),\n",
    "    'default_payment_next_month': [0, 1]\n",
    "}\n",
    "\n",
    "# Checking for unexpected values\n",
    "print(\"Checking for inconsistencies in categorical variables:\")\n",
    "for col in categorical_columns:\n",
    "    unique_values = df[col].unique()\n",
    "    unexpected = [x for x in unique_values if x not in expected_ranges[col]]\n",
    "    if unexpected:\n",
    "        print(f\"{col}: Unexpected values found - {unexpected}\")\n",
    "        print(f\"Count of unexpected values: {len(df[df[col].isin(unexpected)])}\")\n",
    "    else:\n",
    "        print(f\"{col}: All values within expected ranges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7284c5",
   "metadata": {},
   "source": [
    "- All values are within expected ranges based on the documentation and assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb460ab",
   "metadata": {},
   "source": [
    "## **1.3 Handling Duplicate Rows**\n",
    "- There are no duplicate rows in the dataset as we have `ID` column which is unique for each row.\n",
    "- We will drop the `ID` column as it is not needed for modeling and will not contribute to the predictive power of the model.\n",
    "- After dropping the ID column, we will once again check for duplicates to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ed1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the ID column\n",
    "df.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "# Checking for duplicates after dropping ID column\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29387a8",
   "metadata": {},
   "source": [
    "- We have 35 duplicate rows in the dataset, which is very less compared to the total number of rows (30,000). So we will drop these duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ed1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Checking the shape of the DataFrame after removing duplicates and dropping ID column\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba4211",
   "metadata": {},
   "source": [
    "- After cleaning the dataset now we have 29,965 rows and 24 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaccf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned DataFrame to a new CSV file\n",
    "cleaned_data_path = os.path.join(data_dir, \"cleaned_credit_card_data.csv\")\n",
    "df.to_csv(cleaned_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b0327",
   "metadata": {},
   "source": [
    "## **Summary of Data Cleaning**\n",
    "- **Missing Values**: \n",
    "    - We handled missing values 0s in Education_Level and Marital_Status by replacing them with 5 (unknown) and the mode respectively.\n",
    "- **Handling Outliers**:\n",
    "    - We decided not to remove outliers as it would result in losing more than 50% of the data. Instead, we can apply log transformation for skewed numerical variables or use tree-based models which are robust to skewed distributions and outliers.\n",
    "- **Inconsistencies**:\n",
    "    - We replaced 6(unknown) in Education_Level with 5(unknown) as both are (unknown) according to the documentation and as per our assumption to maintain consistency.\n",
    "- **Handling Duplicate Rows**:\n",
    "    - We dropped the `ID` column as it is not needed for modeling and will not contribute to the predictive power of the model.\n",
    "    - After dropping the ID column, we checked for duplicates and found 35 duplicate rows, which is very less compared to the total number of rows (30,000). So we dropped these duplicate rows.\n",
    "- **Data Integrity**:\n",
    "    - The dataset is now cleaned and ready for analysis, with no missing values, inconsistencies, or unexpected values in categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ac548",
   "metadata": {},
   "source": [
    "# **2. Handling Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57db1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers in numerical variables\n",
    "\n",
    "# Creating a copy if df to avoid modifying the original DataFrame\n",
    "df1 = df.copy()\n",
    "\n",
    "for col in numerical_columns:\n",
    "    outlier_count, lower, upper = detect_outliers(df1, col)\n",
    "    df1 = df1[(df1[col] >= lower) & (df1[col] <= upper)]\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Printing the shape of df1 after removing outliers\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d1ae3",
   "metadata": {},
   "source": [
    "- If we remove outliers, we will lose more than 50% of the data, so we will not remove outliers. Instead, we can apply log transformation for skewed numerical variables or use tree-based models which are robust to skewed distributions and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad018fd",
   "metadata": {},
   "source": [
    "# **3. Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21343f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the cleaned DataFrame and saving it to a new variable df1\n",
    "df1 = pd.read_csv(cleaned_data_path)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ed7fb",
   "metadata": {},
   "source": [
    "## **3.1 Binning Age Variable**\n",
    "- We will bin the `Age` variable into age groups to reduce noise and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da35768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the Age variable into age groups\n",
    "df1['Age_Groups'] = pd.cut(df1['Age'],\n",
    "                             bins=[25, 30, 35, 40, 45, 50, 55, 60, np.inf],\n",
    "                             labels=['20-25', '25-30', '30-35', '35-40', '40-45', '45-50', '50-55', '55-60', '60+'],\n",
    "                             right=False)\n",
    "\n",
    "# Dropping the Age column as it is no longer needed\n",
    "df1.drop(columns=['Age'], inplace=True)\n",
    "\n",
    "# Displaying the counts of each age groups\n",
    "df1['Age_Groups'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9facdd",
   "metadata": {},
   "source": [
    "- We grouped the Age variable into age groups to capture more meaningful patterns instead of using it as a continuous variable.\n",
    "- **Insights**: Most of the credit card owners are in the age group of 25-30, followed by 30-35 and 35-40. There are very few credit card owners in the age groups above 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for Age Groups vs Default Payment\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df1, x='Age_Groups', hue='default_payment_next_month', palette=['lightgreen', 'salmon'])\n",
    "plt.title('Age Groups vs Default Payment Next Month', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Age Groups', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.legend(title='Default Payment Next Month', loc='upper right', labels=['No Default', 'Default'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualization_outputs, 'age_groups_vs_default_payment.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44feb357",
   "metadata": {},
   "source": [
    "- Majority of the credit card owners are in the age group of 25-30, followed by 30-35 and 35-40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating ratios of default payment next month by age groups\n",
    "age_ratios = df1.groupby('Age_Groups')[target_column].value_counts(normalize=True).unstack().fillna(0) * 100\n",
    "age_ratios = age_ratios.rename(columns={0: 'No Default', 1: 'Default'})\n",
    "age_ratios['Total_Customers'] = df1['Age_Groups'].value_counts()\n",
    "\n",
    "# Displaying the ratios\n",
    "age_ratios.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5199aaae",
   "metadata": {},
   "source": [
    "- As we can see from the above table, age above 60 has the highest chance of defaulting on payment next month (around 30%), followed by age group 25-30 (around 27%). Ages between 25-50 have a lower chance of defaulting, with the lowest chance in the age group 30-35 (around 20%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0369d",
   "metadata": {},
   "source": [
    "## **3.2 Creating New Features**\n",
    "- We will create new features based on the existing features to improve the predictive power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 5 new features based on existing data\n",
    "\n",
    "# Calculating average bill amount over 6 months\n",
    "df1['Avg_Bill_Amt'] = df1[['Sept_Bill_Amt', 'Aug_Bill_Amt', 'July_Bill_Amt', 'June_Bill_Amt', 'May_Bill_Amt', 'Apr_Bill_Amt']].mean(axis=1).round(2)\n",
    "\n",
    "# Calculating average payment amount over 6 months\n",
    "df1['Avg_Pay_Amt'] = df1[['Sept_Pay_Amt', 'Aug_Pay_Amt', 'July_Pay_Amt', 'June_Pay_Amt', 'May_Pay_Amt', 'Apr_Pay_Amt']].mean(axis=1).round(2)\n",
    "\n",
    "# Calculating payment-to-bill ratio (average payment / average bill, clipped to avoid division by zero)\n",
    "df1['Pay_to_Bill_Ratio'] = np.where(df1['Avg_Bill_Amt'] != 0, df1['Avg_Pay_Amt'] / df1['Avg_Bill_Amt'], 0).round(2)\n",
    "\n",
    "# Calculating average payment delay score (average of payment status)\n",
    "df1['Avg_Delay_Score'] = df1[['Sept_Pay', 'Aug_Pay', 'July_Pay', 'June_Pay', 'May_Pay', 'Apr_Pay']].mean(axis=1).round(2)\n",
    "\n",
    "# Calculating credit utilization ratio (average bill / credit limit)\n",
    "df1['Credit_Utilization'] = np.where(df1['Credit_Limit'] != 0, df1['Avg_Bill_Amt'] / df1['Credit_Limit'], 0).round(2)\n",
    "\n",
    "# Displaying random sample rows of new features created\n",
    "new_features = ['Avg_Bill_Amt', 'Avg_Pay_Amt', 'Pay_to_Bill_Ratio', 'Avg_Delay_Score', 'Credit_Utilization']\n",
    "df1[new_features].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8678c64",
   "metadata": {},
   "source": [
    "- We created 5 new features based on existing variables:\n",
    "    - **Avg_Bill_Amt**: Average of all bill amounts over the six months.\n",
    "    - **Avg_Pay_Amt**: Average of all payment amounts over the six months.\n",
    "    - **Pay_to_Bill_Ratio**: Ratio of average payment amount to average bill amount, indicating how much of the billed amount is paid.\n",
    "    - **Avg_Pay_Delay**: Average payment delay across the six months, indicating overall payment behavior.\n",
    "    - **Credit_Utilization**: Ratio of average bill amount to credit limit, indicating how much of the credit limit is utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first few rows of the updated DataFrame with new features\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the DataFrame after adding new features\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4b37f",
   "metadata": {},
   "source": [
    "- Our new dataset now has 29,965 rows and 29 columns after adding the new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b91ca",
   "metadata": {},
   "source": [
    "### **Summary of New Created Features**\n",
    "- We grouped the Age variable into age groups to capture more meaningful patterns instead of using it as a continuous variable.\n",
    "- We created 5 new features based on existing variables:\n",
    "    - **Avg_Bill_Amt**: Average of all bill amounts over the six months.\n",
    "    - **Avg_Pay_Amt**: Average of all payment amounts over the six months.\n",
    "    - **Pay_to_Bill_Ratio**: Ratio of average payment amount to average bill amount, indicating how much of the billed amount is paid.\n",
    "    - **Avg_Pay_Delay**: Average payment delay across the six months, indicating overall payment behavior.\n",
    "    - **Credit_Utilization**: Ratio of average bill amount to credit limit, indicating how much of the credit limit is utilized.\n",
    "\n",
    "- **Insights**\n",
    "    - Most of the credit card owners are in the age group of 25-30, followed by 30-35 and 35-40. There are very few credit card owners in the age groups above 60.\n",
    "    - Age above 60 has the highest chance of defaulting on payment next month (around 30%), followed by age group 25-30 (around 27%). Ages between 25-50 have a lower chance of defaulting, with the lowest chance in the age group 30-35 (around 20%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d62d05",
   "metadata": {},
   "source": [
    "# **4. Feature Scaling**\n",
    "- We will scale the numerical features using StandardScaler to ensure that all features are on the same scale, which is important for many machine learning algorithms that rely on distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c3e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initializing the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Selecting numerical columns for scaling\n",
    "numerical_columns = ['Credit_Limit', 'Avg_Bill_Amt', 'Avg_Pay_Amt', 'Pay_to_Bill_Ratio', 'Avg_Delay_Score', 'Credit_Utilization']\n",
    "\n",
    "# Fitting the scaler to the numerical columns and transforming them\n",
    "df1[numerical_columns] = scaler.fit_transform(df1[numerical_columns])\n",
    "\n",
    "# Displaying the first few rows of the DataFrame after scaling\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da5f19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c2f44a2",
   "metadata": {},
   "source": [
    "# **5. Encoding Categorical Variables**\n",
    "- We will encode ordinal categorical variables using OrdinalEncoder and nominal categorical variables using OneHotEncoder to prepare them for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba657dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OrdinalEncoder for Age Category\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Define the categories in the correct order (youngest to oldest)\n",
    "age_groups_ordered = [['18-20', '20-25', '25-30', '30-35', '35-40', '40-45', '45-50', '50-55', '55-60', '60+']]\n",
    "\n",
    "# Initializing the OrdinalEncoder with the specified category order\n",
    "ordinal_encoder = OrdinalEncoder(categories=age_groups_ordered)\n",
    "\n",
    "# Applying ordinal encoding to the Age_Groups column\n",
    "df1['Age_Groups'] = ordinal_encoder.fit_transform(df1[['Age_Groups']])\n",
    "\n",
    "# Converting to integer type for cleaner display\n",
    "df1['Age_Groups'] = df1['Age_Groups'].astype(int)\n",
    "\n",
    "# Displaying the mapping to verify\n",
    "for i, category in enumerate(ordinal_encoder.categories_[0]):\n",
    "    print(f\"{category}: {i}\")\n",
    "\n",
    "# Displaying the updated Age_Groups column\n",
    "df1['Age_Groups'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd1a5d",
   "metadata": {},
   "source": [
    "- Mapped Age Groups categories to numerical value using OrdinalEncoder for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae035a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal categories for categorical variables\n",
    "nominal_categories = ['Gender', 'Marital_Status', 'Education_Level', 'Sept_Pay', 'Aug_Pay', 'July_Pay', 'June_Pay', 'May_Pay', 'Apr_Pay']\n",
    "\n",
    "# Transforming nominal categorical variables into numerical values using one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "# Applying one-hot encoding to the nominal categorical variables\n",
    "encoded_features = encoder.fit_transform(df1[nominal_categories])\n",
    "\n",
    "# Creating a DataFrame from the encoded features\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(nominal_categories))\n",
    "\n",
    "# Concatenating the encoded DataFrame with the original DataFrame (excluding the original nominal columns)\n",
    "df1 = pd.concat([df1.drop(columns=nominal_categories), encoded_df], axis=1)\n",
    "\n",
    "# Displaying the first few rows of the updated DataFrame with one-hot encoded features\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e8561",
   "metadata": {},
   "source": [
    "- We have transformed all the nominal categorical variables into numerical values using one-hot encoding converting them into binary columns. This will allow us to use these variables in our machine learning models effectively.\n",
    "- We have also dropped 1 column from one-hot encoding to avoid the dummy variable trap, as it is not needed for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817653aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving default_payment_next_month column to the last position for better clarity in analysis\n",
    "cols = [col for col in df1.columns if col != 'default_payment_next_month']\n",
    "cols.append('default_payment_next_month')\n",
    "df1 = df1[cols]\n",
    "\n",
    "# Display the DataFrame to confirm the change\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14012943",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d28b6",
   "metadata": {},
   "source": [
    "- Now our dataset has 29965 rows and 85 columns after encoding the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the updated DataFrame with new features to a new CSV file\n",
    "final_data_path = os.path.join(data_dir, \"final_data.csv\")\n",
    "df1.to_csv(final_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb91d3",
   "metadata": {},
   "source": [
    "## **Summary of Feature Engineering**\n",
    "- **New Features Created**:\n",
    "    - We created 5 new features based on existing variables:\n",
    "        - **Avg_Bill_Amt**: Average of all bill amounts over the six months.\n",
    "        - **Avg_Pay_Amt**: Average of all payment amounts over the six months.\n",
    "        - **Pay_to_Bill_Ratio**: Ratio of average payment amount to average bill amount, indicating how much of the billed amount is paid.\n",
    "        - **Avg_Pay_Delay**: Average payment delay across the six months, indicating overall payment behavior.\n",
    "        - **Credit_Utilization**: Ratio of average bill amount to credit limit, indicating how much of the credit limit is utilized.\n",
    "- **Age Grouping**:\n",
    "    - We grouped the Age variable into age groups to capture more meaningful patterns instead of using it as a continuous variable.\n",
    "    - **Insights from Age Groups**\n",
    "        - Most of the credit card owners are in the age group of 25-30, followed by 30-35 and 35-40. There are very few credit card owners in the age groups above 60.\n",
    "        - Age above 60 has the highest chance of defaulting on payment next month (around 30%), followed by age group 25-30 (around 27%). Ages between 25-50 have a lower chance of defaulting, with the lowest chance in the age group 30-35 (around 20%).\n",
    "- **Feature Transformation**:\n",
    "    - We encoded ordinal categorical variables using OrdinalEncoder and nominal categorical variables using OneHotEncoder to prepare them for modeling.\n",
    "- **Final Dataset**:\n",
    "    - The final dataset has 29,965 rows and 85 columns after adding new features and encoding categorical variables, ready for modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
