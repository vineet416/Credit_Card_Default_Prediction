{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206116a7",
   "metadata": {},
   "source": [
    "# **1. Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c951b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the final DataFrame\n",
    "df = pd.read_csv(final_data_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d530e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the final DataFrame\n",
    "df2 = pd.read_csv(final_data_path)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93067ba7",
   "metadata": {},
   "source": [
    "## **5.1 Splitting the Dataset**\n",
    "- We will split the dataset into training and testing sets using an 75-25 split to ensure the model is trained on a substantial portion of the data while retaining a separate test set for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f6b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into features and target variable\n",
    "X = df2.drop(columns=['default_payment_next_month'])\n",
    "y = df2['default_payment_next_month']\n",
    "\n",
    "# Displaying the shapes of features and target variable\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd9d8f",
   "metadata": {},
   "source": [
    "- We split the dataset into features (X) and target variable (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eebe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Displaying the shapes of training and testing sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127c82a",
   "metadata": {},
   "source": [
    "- We used stratified sampling to ensure that the class distribution in the target variable is maintained in both training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6e4f5",
   "metadata": {},
   "source": [
    "## **5.2 Handling Class Imbalance on Training Dataset**\n",
    "- We will combining oversampling and undersampling techniques to handle class imbalance in the training dataset.\n",
    "    - We will use the `SMOTE` (Synthetic Minority Over-sampling Technique) to oversample the minority class (default payment). It generates synthetic samples for the minority class by interpolating between existing minority class samples, effectively increasing the representation of the minority class in the training dataset.\n",
    "    - We will use the `Tomek Links` undersampling technique to undersample the majority class (no default payment). Tomek Links identifies and removes samples from the majority class that are close to the decision boundary, helping to clean up the majority class and improve the model's ability to distinguish between classes.\n",
    "- This combination of oversampling the minority class and undersampling the majority class will help us create a balanced dataset for training the model, improving its ability to predict both classes effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying SMOTE to handle class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Displaying the shapes of resampled training set\n",
    "print(f\"X_train_resampled shape: {X_train_resampled.shape}\")\n",
    "print(f\"y_train_resampled shape: {y_train_resampled.shape}\")\n",
    "\n",
    "# Displaying class distribution in original and resampled training sets\n",
    "from collections import Counter\n",
    "print(\"Class distribution in original training set:\", Counter(y_train))\n",
    "print(f\"Class distribution in resampled training set: {Counter(y_train_resampled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c522cd",
   "metadata": {},
   "source": [
    "- After applying SMOTE oversampling, now we have 37336 rows in the training dataset, with 18668 rows in both majority class (no default payment) and minority class (default payment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Tomek Links Under-sampling technique to the resampled training set\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "tomek_links = TomekLinks()\n",
    "X_train_final, y_train_final = tomek_links.fit_resample(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Displaying the shapes of final training set after Tomek Links\n",
    "print(f\"X_train_final shape: {X_train_final.shape}\")\n",
    "print(f\"y_train_final shape: {y_train_final.shape}\")\n",
    "\n",
    "# Displaying class distribution in SMOTE resampled and final training sets\n",
    "print(f\"Class distribution in SMOTE resampled training set: {Counter(y_train_resampled)}\")\n",
    "print(\"Class distribution in final training set:\", Counter(y_train_final))\n",
    "\n",
    "# Saving X_train_final and y_train_final back to X_train and y_train\n",
    "X_train, y_train = X_train_final, y_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42fde6",
   "metadata": {},
   "source": [
    "- After applying Tomek Links undersampling, now we have 36804 rows in the training dataset, with 18668 rows in the majority class (no default payment) and 18136 rows in the minority class (default payment). This ensures a balanced dataset for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e3fa3",
   "metadata": {},
   "source": [
    "## **5.3 Baseline Model**\n",
    "- First we will build a baseline model using Logistic Regression to establish a performance benchmark for our credit card default prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a baseline model using Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initializing the Logistic Regression model\n",
    "logistic_model = LogisticRegression(penalty='l1', max_iter=1000, solver='saga', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fitting the model on the training data\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on training set and test set\n",
    "logistic_y_train_pred = logistic_model.predict(X_train)\n",
    "logistic_y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculating accuracy on training set and test set\n",
    "logistic_train_accuracy = accuracy_score(y_train, logistic_y_train_pred)\n",
    "logistic_test_accuracy = accuracy_score(y_test, logistic_y_pred)\n",
    "\n",
    "# Displaying results for training sets\n",
    "print(f\"Training Accuracy: {logistic_train_accuracy:.4f}\")\n",
    "print(classification_report(y_train, logistic_y_train_pred))\n",
    "print(confusion_matrix(y_train, logistic_y_train_pred))\n",
    "print()\n",
    "\n",
    "# Displaying results for test sets\n",
    "print(\"\\nResults on Test Sets:\")\n",
    "print(f\"Test Accuracy: {logistic_test_accuracy:.4f}\")\n",
    "print(classification_report(y_test, logistic_y_pred))\n",
    "print(confusion_matrix(y_test, logistic_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087a4b5",
   "metadata": {},
   "source": [
    "- The performance of logistic regression model on both training and testing dataset is not very good. The confusion matrix shows that the model is predicting more false positives than false negatives, indicating that it is not able to correctly identify the minority class (default payment) effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5b6d7",
   "metadata": {},
   "source": [
    "## **5.4 Advanced Models**\n",
    "- Now we will build advanced models such as Decision Trees, Random Forest, Gradient Boosting, and XGBoost to improve the performance of our credit card default prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a5513",
   "metadata": {},
   "source": [
    "### **Decision Tree Classifier**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
